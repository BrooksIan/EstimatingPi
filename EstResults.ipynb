{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling To Estimate Pi\n",
    "\n",
    "![RS](PI.png \"Pi\")\n",
    "\n",
    "* Languages:    Python & R\n",
    "* Requirements: Spark\n",
    "* Author: Ian Brooks\n",
    "* Follow: [LinkedIn - Ian Brooks PhD](https://www.linkedin.com/in/ianrbrooksphd/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Python Libs with PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Requirement already up-to-date: plotly in ./.local/lib/python2.7/site-packages (4.9.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python2.7/site-packages (from plotly) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in ./.local/lib/python2.7/site-packages (from plotly) (1.3.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python2.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly --upgrade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import trim\n",
    "import pandas as pd\n",
    "import cdsw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis.r\t      RandomSampleEstPi.ipynb  RS_PySpark8Partions.py\r\n",
      "config.yml\t      README.md\t\t       RS_PYSpark_estimatePi.py\r\n",
      "entry.R\t\t      Results.ipynb\t       RS_scikitlearn.py\r\n",
      "MonteCarloGraph.jpeg  RS_estimatePi.py\t       server.R\r\n",
      "pi_experiments.json   RS_estimatePi.R\t       ui.R\r\n",
      "PI.png\t\t      RS_PySpark16Partions.py\r\n",
      "R\t\t      RS_PySpark4Partions.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS: Create Directory & Copy JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/tmp/pi_experiments.json': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -put  pi_experiments.json /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalize Spark Session & Check Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalize Spark Session \n",
    "spark = SparkSession.builder \\\n",
    "      .appName(\"Review Pi Estimations\") \\\n",
    "      .config('spark.shuffle.service.enabled',\"True\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0.7.1.0.0-714'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Dataframe from File\n",
    "raw_data = spark.read.json(\"pi_experiments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
